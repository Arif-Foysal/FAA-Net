{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIIA-IDS: Research Paper Deliverables Generation\n",
    "\n",
    "This notebook generates the tables and figures required for the research paper. It assumes that the training scripts (`train_main.py`, `run_ablation.py`, `train_baselines.py`) have been executed and their artifacts saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clone Repository\n",
    "# TODO: Replace with your actual repository URL\n",
    "GIT_REPO_URL = \"https://github.com/Arif-Foysal/FAA-Net.git\"\n",
    "REPO_DIR = \"FAA-Net\" # This usually matches the name of the git repo\n",
    "\n",
    "!git clone {GIT_REPO_URL}\n",
    "\n",
    "import os\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"Changed directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find directory {REPO_DIR}. Check if git clone succeeded.\")\n",
    "\n",
    "# 2. Mount Google Drive (for saving models)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab, skipping Drive mount.\")\n",
    "\n",
    "# 3. Install Dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from core.config import DROPPED_FEATURES\n",
    "from core.data_loader import load_and_preprocess_data\n",
    "from core.data_loader import get_data_paths\n",
    "\n",
    "# Setup output directory for figures\n",
    "FIG_DIR = '../paper_figures'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Check for Google Drive path if useful, otherwise check local and parent dirs\n",
    "base_dir = '.'\n",
    "if os.path.exists('/content/drive/MyDrive/FAIIA_Models'):\n",
    "    base_dir = '/content/drive/MyDrive/FAIIA_Models'\n",
    "    print(f\"Using artifacts from Google Drive: {base_dir}\")\n",
    "elif os.path.exists('../edan_v3_main.pt'):\n",
    "    base_dir = '..'\n",
    "    print(f\"Found artifacts in parent directory: {os.path.abspath(base_dir)}\")\n",
    "else:\n",
    "    print(f\"Using local artifacts in: {os.path.abspath(base_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data & Preprocessing Artifacts\n",
    "### Table D1 & D2: Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Reload data to get raw stats (relying on cache if available)\n",
    "    data_dir = \"/content\" if os.path.exists(\"/content\") else \"..\"\n",
    "    # Note: We need the test set categorical labels for Per-Attack Analysis later\n",
    "    X_train, X_test, y_train, y_test, y_train_cat, y_test_cat = load_and_preprocess_data(data_dir=data_dir)\n",
    "    \n",
    "    print(\"\\n=== Table D1: Dataset Statistics ===\")\n",
    "    d1 = pd.DataFrame({\n",
    "        'Split': ['Train', 'Test', 'Total'],\n",
    "        'Samples': [len(y_train), len(y_test), len(y_train) + len(y_test)],\n",
    "        'Minority (Attack)': [y_train.sum(), y_test.sum(), y_train.sum() + y_test.sum()],\n",
    "        'Majority (Normal)': [len(y_train)-y_train.sum(), len(y_test)-y_test.sum(), (len(y_train)-y_train.sum()) + (len(y_test)-y_test.sum())]\n",
    "    })\n",
    "    d1['Imbalance Ratio'] = d1['Majority (Normal)'] / d1['Minority (Attack)']\n",
    "    display(d1)\n",
    "    \n",
    "    if y_train_cat is not None:\n",
    "        print(\"\\n=== Table D2: Per-Attack Sample Distribution ===\")\n",
    "        # We need to map encoded values back to names if we had the encoder, \n",
    "        # but for now we will show distribution of encoded classes\n",
    "        train_counts = pd.Series(y_train_cat).value_counts().sort_index()\n",
    "        test_counts = pd.Series(y_test_cat).value_counts().sort_index()\n",
    "        d2 = pd.DataFrame({'Train': train_counts, 'Test': test_counts})\n",
    "        d2['Total'] = d2['Train'] + d2['Test']\n",
    "        display(d2)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load dataset for stats: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Artifacts\n",
    "### Figure F1: FAIIA Architecture Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    Input[Input Features] --> Norm[BatchNorm]\n",
    "    Norm --> ProbEst[Prob Estimator (MLP)]\n",
    "    ProbEst -->|Minority Prob| FAIIA[FAIIA Attention Block]\n",
    "    Norm --> FAIIA\n",
    "    \n",
    "    subgraph FAIIA Block\n",
    "    SelfAtt[Self-Attention] \n",
    "    Proto[Prototype Attention]\n",
    "    Focal[Focal Modulation]\n",
    "    \n",
    "    SelfAtt --> Focal\n",
    "    Proto --> Focal\n",
    "    end\n",
    "\n",
    "    FAIIA --> SE[Squeeze-and-Excitation]\n",
    "    SE --> Res[Residual Hidden Blocks]\n",
    "    Res --> Head[Classifier Head]\n",
    "    Head --> Output[Attack Probability]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure F2: FAIIA vs Vanilla DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Vanilla DNN\n",
    "    I1[Input] --> L1[Linear+BN+ReLU] --> L2[Linear...] --> O1[Logits]\n",
    "    end\n",
    "\n",
    "    subgraph FAIIA-IDS\n",
    "    I2[Input] --> F[FAIIA Attention] --> S[SE Block] --> H[Hidden Blocks] --> O2[Sigmoid Output]\n",
    "    P[Minority Prototypes] -.-> F\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Convergence Evidence\n",
    "### Figure F3-F5: Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = os.path.join(base_dir, 'edan_v3_history.csv')\n",
    "vanilla_history_path = os.path.join(base_dir, 'vanilladnn_history.csv')\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    hist = pd.read_csv(history_path)\n",
    "    epochs = range(1, len(hist) + 1)\n",
    "    \n",
    "    # Try to load Vanilla DNN history\n",
    "    hist_vanilla = None\n",
    "    if os.path.exists(vanilla_history_path):\n",
    "        hist_vanilla = pd.read_csv(vanilla_history_path)\n",
    "        epochs_vanilla = range(1, len(hist_vanilla) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, hist['train_loss'], label='FAIIA Train', color='blue')\n",
    "    axes[0].plot(epochs, hist['val_loss'], label='FAIIA Val', linestyle='--', color='blue')\n",
    "    if hist_vanilla is not None:\n",
    "        axes[0].plot(epochs_vanilla, hist_vanilla['train_loss'], label='Vanilla Train', color='gray', alpha=0.7)\n",
    "        axes[0].plot(epochs_vanilla, hist_vanilla['val_loss'], label='Vanilla Val', linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    axes[0].set_title('Figure 6: Convergence plot (Loss vs Epoch)')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # F1\n",
    "    axes[1].plot(epochs, hist['train_f1'], label='FAIIA Train', color='green')\n",
    "    axes[1].plot(epochs, hist['val_f1'], label='FAIIA Val', linestyle='--', color='green')\n",
    "    if hist_vanilla is not None:\n",
    "        axes[1].plot(epochs_vanilla, hist_vanilla['train_f1'], label='Vanilla Train', color='gray', alpha=0.7)\n",
    "        axes[1].plot(epochs_vanilla, hist_vanilla['val_f1'], label='Vanilla Val', linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    axes[1].set_title('Figure 1: F1-Score vs Epoch')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Recall\n",
    "    axes[2].plot(epochs, hist['train_recall'], label='FAIIA Train', color='red')\n",
    "    axes[2].plot(epochs, hist['val_recall'], label='FAIIA Val', linestyle='--', color='red')\n",
    "    if hist_vanilla is not None:\n",
    "        axes[2].plot(epochs_vanilla, hist_vanilla['train_recall'], label='Vanilla Train', color='gray', alpha=0.7)\n",
    "        axes[2].plot(epochs_vanilla, hist_vanilla['val_recall'], label='Vanilla Val', linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    axes[2].set_title('Recall vs Epoch')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_ylabel('Recall')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_DIR, 'training_curves.png'))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training history not found. Run train_main.py first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Model Performance\n",
    "### Table T1: Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = os.path.join(base_dir, 'edan_v3_metrics.csv')\n",
    "if os.path.exists(metrics_path):\n",
    "    t1 = pd.read_csv(metrics_path)\n",
    "    t1.index = ['FAIIA (EDAN v3)']\n",
    "    display(t1)\n",
    "else:\n",
    "    print(\"Main model metrics not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2 & Figures 4-5: Per-Attack Analysis (Minority vs Majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_loader import get_data_paths\n",
    "# Analyze Performance per Attack Category\n",
    "pred_path = os.path.join(base_dir, 'edan_v3_predictions.npz')\n",
    "\n",
    "if os.path.exists(pred_path):\n",
    "    # 1. Load Predictions\n",
    "    data = np.load(pred_path)\n",
    "    y_true_bin = data['y_true']\n",
    "    y_probs = data['y_probs']\n",
    "    \n",
    "    # 2. Get Attack Categories\n",
    "    _, test_path = get_data_paths(data_dir=\"/content\" if os.path.exists(\"/content\") else \"..\")\n",
    "    \n",
    "    if os.path.exists(test_path):\n",
    "        df_test_raw = pd.read_csv(test_path)\n",
    "        if len(df_test_raw) != len(y_probs):\n",
    "            print(f\"Warning: Test set size ({len(df_test_raw)}) does not match predictions ({len(y_probs)}).\")\n",
    "        else:\n",
    "            analysis_df = pd.DataFrame({\n",
    "                'True Binary': y_true_bin,\n",
    "                'Prob': y_probs,\n",
    "                'Pred Binary': (y_probs > 0.5).astype(int),\n",
    "                'Category': df_test_raw['attack_cat'].fillna('Normal')\n",
    "            })\n",
    "            analysis_df['Category'] = analysis_df['Category'].replace({'Backdoors': 'Backdoor'})\n",
    "            attack_metrics = []\n",
    "            for cat in analysis_df['Category'].unique():\n",
    "                subset = analysis_df[analysis_df['Category'] == cat]\n",
    "                count = len(subset)\n",
    "                detected = subset['Pred Binary'].sum()\n",
    "                attack_metrics.append({\n",
    "                    'Attack': cat,\n",
    "                    'Samples': count,\n",
    "                    'Detection Rate': detected / count,\n",
    "                    'Type': 'Majority' if count >= 5000 else 'Minority'\n",
    "                })\n",
    "            t2 = pd.DataFrame(attack_metrics).sort_values('Samples', ascending=False)\n",
    "            print(\"\\n=== Table 2: Per-attack metrics (Minority < 5000 vs Majority >= 5000) ===\")\n",
    "            display(t2)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=t2[t2['Type'] == 'Minority'], x='Attack', y='Detection Rate', palette='viridis')\n",
    "            plt.title('Figure 4: Minority detection comparison (Recall)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylim(0, 1.1)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(FIG_DIR, 'minority_detection.png'))\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=t2[t2['Type'] == 'Majority'], x='Attack', y='Detection Rate', palette='magma')\n",
    "            plt.title('Figure 5: Majority detection comparison')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylim(0, 1.1)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(FIG_DIR, 'majority_detection.png'))\n",
    "            plt.show()\n",
    "    else:\n",
    "         print(\"Test CSV not found for per-attack analysis.\")\n",
    "else:\n",
    "    print(\"Predictions not found for per-attack analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classical Baseline Comparisons\n",
    "### Table T2: Classical ML Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = 'baseline_summary.csv'\n",
    "# Check Drive location too\n",
    "if not os.path.exists(baseline_path) and os.path.exists(os.path.join(base_dir, 'baseline_summary.csv')):\n",
    "    baseline_path = os.path.join(base_dir, 'baseline_summary.csv')\n",
    "\n",
    "if os.path.exists(baseline_path):\n",
    "    t2 = pd.read_csv(baseline_path, index_col=0)\n",
    "    display(t2)\n",
    "else:\n",
    "    print(\"Baseline results not found. Run train_baselines.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Ablation Study\n",
    "### Table T3: FAIIA Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_path = 'ablation_summary.csv'\n",
    "if not os.path.exists(ablation_path) and os.path.exists(os.path.join(base_dir, 'ablation_summary.csv')):\n",
    "    ablation_path = os.path.join(base_dir, 'ablation_summary.csv')\n",
    "\n",
    "if os.path.exists(ablation_path):\n",
    "    t3 = pd.read_csv(ablation_path, index_col=0)\n",
    "    t3['Attention'] = t3.index.str.contains('FAIIA')\n",
    "    t3['Prototypes'] = t3.index.str.contains('FAIIA')\n",
    "    t3['Focal'] = t3.index.str.contains('Focal')\n",
    "    cols = ['Attention', 'Prototypes', 'Focal'] + [c for c in t3.columns if c not in ['Attention', 'Prototypes', 'Focal']]\n",
    "    t3 = t3[cols]\n",
    "    display(t3)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if 'F1-Score' in t3.columns:\n",
    "        plot_df = t3.sort_values('F1-Score')\n",
    "        sns.barplot(x=plot_df.index, y=plot_df['F1-Score'], palette='coolwarm')\n",
    "        plt.title('Figure 7: Ablation comparison')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.xlabel('Model Variant')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0.8, 1.0)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIG_DIR, 'ablation_comparison.png'))\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Ablation results not found. Run run_ablation.py first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis Plots\n",
    "### Figure F7 & F8: ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(models_dict, curve_type='ROC'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, file_path in models_dict.items():\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        data = np.load(file_path)\n",
    "        y_true = data['y_true']\n",
    "        y_probs = data['y_probs']\n",
    "        \n",
    "        if curve_type == 'ROC':\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Figure 3: ROC curves per model')\n",
    "        else:\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "            plt.plot(recall, precision, label=f'{name}')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Figure 2: PR curves per model')\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(FIG_DIR, f'{curve_type.lower()}_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Define prediction files\n",
    "pred_files = {\n",
    "    'FAIIA (Full)': os.path.join(base_dir, 'edan_v3_predictions.npz'),\n",
    "    'Vanilla DNN': os.path.join(base_dir, 'vanilladnn_focal_predictions.npz'),\n",
    "    'XGBoost': os.path.join(base_dir, 'xgboost_predictions.npz'),\n",
    "    'LightGBM': os.path.join(base_dir, 'lightgbm_predictions.npz')\n",
    "}\n",
    "\n",
    "print(\"Plotting ROC Curves...\")\n",
    "plot_curves(pred_files, 'ROC')\n",
    "\n",
    "print(\"Plotting PR Curves...\")\n",
    "plot_curves(pred_files, 'PR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Efficiency\n",
    "### Table A3: Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to count parameters properly\n",
    "from core.model import EDANv3\n",
    "from core.ablation import VanillaDNN_Ablation\n",
    "from core.config import V3_CONFIG\n",
    "\n",
    "# Mock input dim\n",
    "input_dim = 40 # approx\n",
    "\n",
    "dnn = VanillaDNN_Ablation(input_dim=input_dim).to('cpu')\n",
    "faiia = EDANv3(input_dim=input_dim, \n",
    "               num_heads=V3_CONFIG['num_heads'], \n",
    "               attention_dim=V3_CONFIG['attention_dim'],\n",
    "               n_prototypes=V3_CONFIG['n_prototypes']).to('cpu')\n",
    "\n",
    "efficiency_df = pd.DataFrame({\n",
    "    'Model': ['Vanilla DNN', 'FAIIA (EDAN v3)'],\n",
    "    'Parameters': [dnn.count_parameters(), faiia.count_parameters()],\n",
    "    'Inference': ['Fast', 'Moderate'] # Placeholder\n",
    "})\n",
    "display(efficiency_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}