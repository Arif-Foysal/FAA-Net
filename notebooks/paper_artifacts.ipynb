
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIIA-IDS: Research Paper Deliverables Generation\n",
    "\n",
    "This notebook generates the tables and figures required for the research paper. It assumes that the training scripts (`train_main.py`, `run_ablation.py`, `train_baselines.py`) have been executed and their artifacts saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from core.config import DROPPED_FEATURES\n",
    "from core.data_loader import load_and_preprocess_data\n",
    "\n",
    "# Setup output directory for figures\n",
    "FIG_DIR = '../paper_figures'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Check for Google Drive path if useful\n",
    "base_dir = '.'\n",
    "if os.path.exists('/content/drive/MyDrive/FAIIA_Models'):\n",
    "    base_dir = '/content/drive/MyDrive/FAIIA_Models'\n",
    "    print(f\"Using artifacts from Google Drive: {base_dir}\")\n",
    "else:\n",
    "    print(f\"Using local artifacts in: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data & Preprocessing Artifacts\n",
    "### Table D1 & D2: Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Reload data to get raw stats (relying on cache if available)\n",
    "    data_dir = \"/content\" if os.path.exists(\"/content\") else \"..\"\n",
    "    # Note: We need the test set categorical labels for Per-Attack Analysis later\n",
    "    X_train, X_test, y_train, y_test, y_train_cat, y_test_cat = load_and_preprocess_data(data_dir=data_dir)\n",
    "    \n",
    "    print(\"\\n=== Table D1: Dataset Statistics ===\")\n",
    "    d1 = pd.DataFrame({\n",
    "        'Split': ['Train', 'Test', 'Total'],\n",
    "        'Samples': [len(y_train), len(y_test), len(y_train) + len(y_test)],\n",
    "        'Minority (Attack)': [y_train.sum(), y_test.sum(), y_train.sum() + y_test.sum()],\n",
    "        'Majority (Normal)': [len(y_train)-y_train.sum(), len(y_test)-y_test.sum(), (len(y_train)-y_train.sum()) + (len(y_test)-y_test.sum())]\n",
    "    })\n",
    "    d1['Imbalance Ratio'] = d1['Majority (Normal)'] / d1['Minority (Attack)']\n",
    "    display(d1)\n",
    "    \n",
    "    if y_train_cat is not None:\n",
    "        print(\"\\n=== Table D2: Per-Attack Sample Distribution ===\")\n",
    "        # We need to map encoded values back to names if we had the encoder, \n",
    "        # but for now we will show distribution of encoded classes\n",
    "        train_counts = pd.Series(y_train_cat).value_counts().sort_index()\n",
    "        test_counts = pd.Series(y_test_cat).value_counts().sort_index()\n",
    "        d2 = pd.DataFrame({'Train': train_counts, 'Test': test_counts})\n",
    "        d2['Total'] = d2['Train'] + d2['Test']\n",
    "        display(d2)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load dataset for stats: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Artifacts\n",
    "### Figure F1: FAIIA Architecture Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    Input[Input Features] --> Norm[BatchNorm]\n",
    "    Norm --> ProbEst[Prob Estimator (MLP)]\n",
    "    ProbEst -->|Minority Prob| FAIIA[FAIIA Attention Block]\n",
    "    Norm --> FAIIA\n",
    "    \n",
    "    subgraph FAIIA Block\n",
    "    SelfAtt[Self-Attention] \n",
    "    Proto[Prototype Attention]\n",
    "    Focal[Focal Modulation]\n",
    "    \n",
    "    SelfAtt --> Focal\n",
    "    Proto --> Focal\n",
    "    end\n",
    "\n",
    "    FAIIA --> SE[Squeeze-and-Excitation]\n",
    "    SE --> Res[Residual Hidden Blocks]\n",
    "    Res --> Head[Classifier Head]\n",
    "    Head --> Output[Attack Probability]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure F2: FAIIA vs Vanilla DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Vanilla DNN\n",
    "    I1[Input] --> L1[Linear+BN+ReLU] --> L2[Linear...] --> O1[Logits]\n",
    "    end\n",
    "\n",
    "    subgraph FAIIA-IDS\n",
    "    I2[Input] --> F[FAIIA Attention] --> S[SE Block] --> H[Hidden Blocks] --> O2[Sigmoid Output]\n",
    "    P[Minority Prototypes] -.-> F\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Convergence Evidence\n",
    "### Figure F3-F5: Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = os.path.join(base_dir, 'edan_v3_history.csv')\n",
    "if os.path.exists(history_path):\n",
    "    hist = pd.read_csv(history_path)\n",
    "    epochs = range(1, len(hist) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, hist['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(epochs, hist['val_loss'], label='Val Loss')\n",
    "    axes[0].set_title('Figure F3: Loss Convergence')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # F1\n",
    "    axes[1].plot(epochs, hist['train_f1'], label='Train F1')\n",
    "    axes[1].plot(epochs, hist['val_f1'], label='Val F1')\n",
    "    axes[1].set_title('Figure F4: F1-Score Convergence')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Recall\n",
    "    axes[2].plot(epochs, hist['train_recall'], label='Train Recall')\n",
    "    axes[2].plot(epochs, hist['val_recall'], label='Val Recall')\n",
    "    axes[2].set_title('Figure F5: Recall Convergence')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_ylabel('Recall')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_DIR, 'training_curves.png'))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training history not found. Run train_main.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Model Performance\n",
    "### Table T1: Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = os.path.join(base_dir, 'edan_v3_metrics.csv')\n",
    "if os.path.exists(metrics_path):\n",
    "    t1 = pd.read_csv(metrics_path)\n",
    "    t1.index = ['FAIIA (EDAN v3)']\n",
    "    display(t1)\n",
    "else:\n",
    "    print(\"Main model metrics not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classical Baseline Comparisons\n",
    "### Table T2: Classical ML Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = 'baseline_summary.csv'\n",
    "# Check Drive location too\n",
    "if not os.path.exists(baseline_path) and os.path.exists(os.path.join(base_dir, 'baseline_summary.csv')):\n",
    "    baseline_path = os.path.join(base_dir, 'baseline_summary.csv')\n",
    "\n",
    "if os.path.exists(baseline_path):\n",
    "    t2 = pd.read_csv(baseline_path, index_col=0)\n",
    "    display(t2)\n",
    "else:\n",
    "    print(\"Baseline results not found. Run train_baselines.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Ablation Study\n",
    "### Table T3: FAIIA Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_path = 'ablation_summary.csv'\n",
    "# Check Drive location too\n",
    "if not os.path.exists(ablation_path) and os.path.exists(os.path.join(base_dir, 'ablation_summary.csv')):\n",
    "    ablation_path = os.path.join(base_dir, 'ablation_summary.csv')\n",
    "\n",
    "if os.path.exists(ablation_path):\n",
    "    t3 = pd.read_csv(ablation_path, index_col=0)\n",
    "    # Add columns for components to match paper format\n",
    "    t3['Attention'] = t3.index.str.contains('FAIIA')\n",
    "    t3['Prototypes'] = t3.index.str.contains('FAIIA')\n",
    "    t3['Focal'] = t3.index.str.contains('Focal')\n",
    "    \n",
    "    # Reorder\n",
    "    cols = ['Attention', 'Prototypes', 'Focal'] + [c for c in t3.columns if c not in ['Attention', 'Prototypes', 'Focal']]\n",
    "    t3 = t3[cols]\n",
    "    display(t3)\n",
    "else:\n",
    "    print(\"Ablation results not found. Run run_ablation.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis Plots\n",
    "### Figure F7 & F8: ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(models_dict, curve_type='ROC'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, file_path in models_dict.items():\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        data = np.load(file_path)\n",
    "        y_true = data['y_true']\n",
    "        y_probs = data['y_probs']\n",
    "        \n",
    "        if curve_type == 'ROC':\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Figure F8: ROC Comparison')\n",
    "        else:\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "            plt.plot(recall, precision, label=f'{name}')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Figure F7: Precision-Recall Curve')\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(FIG_DIR, f'{curve_type.lower()}_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Define prediction files\n",
    "pred_files = {\n",
    "    'FAIIA (Full)': os.path.join(base_dir, 'edan_v3_predictions.npz'),\n",
    "    'Vanilla DNN': os.path.join(base_dir, 'vanilladnn_focal_predictions.npz'),\n",
    "    'XGBoost': os.path.join(base_dir, 'xgboost_predictions.npz'),\n",
    "    'LightGBM': os.path.join(base_dir, 'lightgbm_predictions.npz')\n",
    "}\n",
    "\n",
    "print(\"Plotting ROC Curves...\")\n",
    "plot_curves(pred_files, 'ROC')\n",
    "\n",
    "print(\"Plotting PR Curves...\")\n",
    "plot_curves(pred_files, 'PR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Efficiency\n",
    "### Table A3: Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to count parameters properly\n",
    "from core.model import EDANv3\n",
    "from core.ablation import VanillaDNN_Ablation\n",
    "from core.config import V3_CONFIG\n",
    "\n",
    "# Mock input dim\n",
    "input_dim = 40 # approx\n",
    "\n",
    "dnn = VanillaDNN_Ablation(input_dim=input_dim).to('cpu')\n",
    "faiia = EDANv3(input_dim=input_dim, \n",
    "               num_heads=V3_CONFIG['num_heads'], \n",
    "               attention_dim=V3_CONFIG['attention_dim'],\n",
    "               n_prototypes=V3_CONFIG['n_prototypes']).to('cpu')\n",
    "\n",
    "efficiency_df = pd.DataFrame({\n",
    "    'Model': ['Vanilla DNN', 'FAIIA (EDAN v3)'],\n",
    "    'Parameters': [dnn.count_parameters(), faiia.count_parameters()],\n",
    "    'Inference': ['Fast', 'Moderate'] # Placeholder\n",
    "})\n",
    "display(efficiency_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
