{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIIA-IDS Ablation Study (Refactored)\n",
    "\n",
    "This notebook runs the ablation study for the FAIIA-IDS model by cloning the refactored codebase from GitHub.\n",
    "\n",
    "**Note:** Please replace `https://github.com/USERNAME/REPO_NAME.git` with your actual repository URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clone Repository\n",
    "# TODO: Replace with your actual repository URL\n",
    "GIT_REPO_URL = \"https://github.com/Arif-Foysal/FAA-Net.git\"\n",
    "REPO_DIR = \"FAA-Net\" # This usually matches the name of the git repo\n",
    "\n",
    "!git clone {GIT_REPO_URL}\n",
    "\n",
    "import os\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"Changed directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find directory {REPO_DIR}. Check if git clone succeeded.\")\n",
    "\n",
    "# 2. Mount Google Drive (for saving models)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab, skipping Drive mount.\")\n",
    "\n",
    "# 3. Install Dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure project root is in path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scripts.run_ablation import main as run_ablation_experiment\n",
    "from scripts.train_main import main as train_main_model\n",
    "from scripts.train_baselines import main as run_baseline_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Main EDAN v3 Model\n",
    "\n",
    "Trains the full EDAN v3 model with FAIIA and Minority Prototypes using the standard configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Run training logic\n",
    "    train_main_model()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure dataset files (UNSW_NB15) are present in the project root or /content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Ablation Study\n",
    "\n",
    "Runs 6 experiments:\n",
    "1. Vanilla DNN + BCE\n",
    "2. Vanilla DNN + Focal Loss\n",
    "3. FAIIA + BCE\n",
    "4. FAIIA + Focal Loss\n",
    "5. **FAIIA + EWKM + BCE** *(NEW — Entropy-Weighted KMeans prototypes)*\n",
    "6. **FAIIA + EWKM + Focal Loss** *(NEW — Entropy-Weighted KMeans prototypes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    run_ablation_experiment()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. EWKM Ablation (Entropy-Weighted KMeans Prototypes)\n",
    "\n",
    "Runs **only** the two new EWKM experiments:\n",
    "1. **FAIIA + EWKM + BCE** — Feature-discriminative prototypes with weighted BCE\n",
    "2. **FAIIA + EWKM + Focal Loss** — Feature-discriminative prototypes with Focal Loss\n",
    "\n",
    "> Requires the data to be loaded (Cell 2) and imports (Cell 3) to have been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from core.config import V3_CONFIG, RANDOM_STATE\n",
    "from core.data_loader import load_and_preprocess_data, create_dataloaders\n",
    "from core.ablation import EDANv3_Ablation\n",
    "from core.loss import ImbalanceAwareFocalLoss_Logits\n",
    "from core.trainer import train_model\n",
    "from core.utils import set_all_seeds, evaluate_model, print_metrics, save_predictions\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ============================================================\n",
    "# EWKM: Only used for better prototype centroid generation\n",
    "# ============================================================\n",
    "class EntropyWeightedKMeans:\n",
    "    \"\"\"\n",
    "    Entropy-Weighted K-Means for feature-discriminative prototype generation.\n",
    "    Produces better centroids than standard KMeans by learning per-cluster\n",
    "    feature importance weights during clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=8, gamma=0.1, max_iter=100, tol=1e-4, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.cluster_centers_ = None\n",
    "        self.feature_weights_ = None\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        kmeans_init = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "        kmeans_init.fit(X)\n",
    "        labels = kmeans_init.labels_\n",
    "        centers = kmeans_init.cluster_centers_.copy()\n",
    "        weights = np.ones((self.n_clusters, n_features)) / n_features\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            old_centers = centers.copy()\n",
    "\n",
    "            # E-step: weighted distance assignment\n",
    "            distances = np.zeros((n_samples, self.n_clusters))\n",
    "            for l in range(self.n_clusters):\n",
    "                diff = X - centers[l]\n",
    "                distances[:, l] = np.sum(weights[l] * (diff ** 2), axis=1)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "\n",
    "            # M-step 1: update centroids\n",
    "            for l in range(self.n_clusters):\n",
    "                members = X[labels == l]\n",
    "                if len(members) > 0:\n",
    "                    centers[l] = members.mean(axis=0)\n",
    "\n",
    "            # M-step 2: update feature weights via entropy regularization\n",
    "            for l in range(self.n_clusters):\n",
    "                members = X[labels == l]\n",
    "                if len(members) > 0:\n",
    "                    dispersions = np.sum((members - centers[l]) ** 2, axis=0) + 1e-10\n",
    "                    log_w = -dispersions / self.gamma\n",
    "                    log_w -= log_w.max()\n",
    "                    weights[l] = np.exp(log_w)\n",
    "                    weights[l] /= weights[l].sum()\n",
    "\n",
    "            if np.linalg.norm(centers - old_centers) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.cluster_centers_ = centers\n",
    "        self.feature_weights_ = weights\n",
    "        self.n_iter_ = iteration + 1\n",
    "        return self\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "V3_EWKM_CONFIG = {**V3_CONFIG, 'use_ewkm': True, 'ewkm_gamma': 0.1, 'patience': 30}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_all_seeds(RANDOM_STATE)\n",
    "\n",
    "# --- Load Data ---\n",
    "data_dir = \"/content\" if os.path.exists(\"/content\") else \".\"\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, _, _ = load_and_preprocess_data(data_dir=data_dir)\n",
    "\n",
    "train_loader, val_loader, _, X_test_tensor = create_dataloaders(\n",
    "    X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "    batch_size=V3_EWKM_CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "count_positive = y_train.sum()\n",
    "count_negative = len(y_train) - count_positive\n",
    "class_counts = [count_negative, count_positive]\n",
    "pos_weight = torch.tensor([count_negative / count_positive], device=device, dtype=torch.float32)\n",
    "\n",
    "# --- Generate EWKM Prototypes ---\n",
    "print(\"Generating EWKM prototypes...\")\n",
    "minority_mask = y_train.values == 1\n",
    "X_minority = X_train_scaled[minority_mask]\n",
    "\n",
    "ewkm = EntropyWeightedKMeans(\n",
    "    n_clusters=V3_EWKM_CONFIG['n_prototypes'],\n",
    "    gamma=V3_EWKM_CONFIG['ewkm_gamma'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "ewkm.fit(X_minority)\n",
    "prototypes_ewkm = ewkm.cluster_centers_\n",
    "\n",
    "print(f\"  Shape: {prototypes_ewkm.shape}\")\n",
    "print(f\"  Gamma: {V3_EWKM_CONFIG['ewkm_gamma']}\")\n",
    "print(f\"  Converged in {ewkm.n_iter_} iterations\")\n",
    "\n",
    "# Diagnostic: verify feature weights are NOT uniform\n",
    "for i in range(min(3, ewkm.feature_weights_.shape[0])):\n",
    "    w = ewkm.feature_weights_[i]\n",
    "    ent = -np.sum(w * np.log(w + 1e-10))\n",
    "    max_ent = np.log(len(w))\n",
    "    ratio = ent / max_ent\n",
    "    top3 = np.argsort(w)[-3:][::-1]\n",
    "    print(f\"  Prototype {i}: entropy_ratio={ratio:.3f} \"\n",
    "          f\"(1.0=uniform, lower=more discriminative), top features: {top3}\")\n",
    "\n",
    "# --- Helper: build model + init with EWKM prototypes using existing API ---\n",
    "def build_ewkm_model():\n",
    "    \"\"\"Build EDANv3_Ablation and initialize prototypes from EWKM centroids.\"\"\"\n",
    "    set_all_seeds(RANDOM_STATE)\n",
    "    model = EDANv3_Ablation(\n",
    "        input_dim=input_dim,\n",
    "        num_heads=V3_EWKM_CONFIG['num_heads'],\n",
    "        attention_dim=V3_EWKM_CONFIG['attention_dim'],\n",
    "        n_prototypes=V3_EWKM_CONFIG['n_prototypes'],\n",
    "    ).to(device)\n",
    "    # Use the EXISTING initialize_all_prototypes API — no monkey-patching\n",
    "    model.faiia.initialize_all_prototypes(prototypes_ewkm, device)\n",
    "    return model\n",
    "\n",
    "ewkm_results = {}\n",
    "\n",
    "# --- Experiment: FAIIA + EWKM + BCE ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment: FAIIA + EWKM + BCE\")\n",
    "print(\"=\"*60)\n",
    "model_ewkm_bce = build_ewkm_model()\n",
    "criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "model_ewkm_bce, hist_bce = train_model(\n",
    "    model_ewkm_bce, train_loader, val_loader, V3_EWKM_CONFIG, criterion_bce, device\n",
    ")\n",
    "metrics_bce, y_probs_bce, y_pred_bce = evaluate_model(model_ewkm_bce, X_test_tensor, y_test, device)\n",
    "print_metrics(metrics_bce, \"FAIIA + EWKM + BCE Results\")\n",
    "save_predictions(y_test, y_probs_bce, \"faiia_ewkm_bce_predictions.npz\")\n",
    "ewkm_results['FAIIA + EWKM + BCE'] = metrics_bce\n",
    "\n",
    "# --- Experiment: FAIIA + EWKM + Focal Loss ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment: FAIIA + EWKM + Focal Loss\")\n",
    "print(\"=\"*60)\n",
    "model_ewkm_focal = build_ewkm_model()\n",
    "criterion_focal = ImbalanceAwareFocalLoss_Logits(class_counts=class_counts, gamma=2.0)\n",
    "\n",
    "model_ewkm_focal, hist_focal = train_model(\n",
    "    model_ewkm_focal, train_loader, val_loader, V3_EWKM_CONFIG, criterion_focal, device\n",
    ")\n",
    "metrics_focal, y_probs_focal, y_pred_focal = evaluate_model(model_ewkm_focal, X_test_tensor, y_test, device)\n",
    "print_metrics(metrics_focal, \"FAIIA + EWKM + Focal Results\")\n",
    "save_predictions(y_test, y_probs_focal, \"faiia_ewkm_focal_predictions.npz\")\n",
    "ewkm_results['FAIIA + EWKM + Focal'] = metrics_focal\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EWKM Ablation Summary\")\n",
    "print(\"=\"*60)\n",
    "df_ewkm = pd.DataFrame(ewkm_results).T\n",
    "display(df_ewkm)\n",
    "\n",
    "if os.path.exists('ablation_summary.csv'):\n",
    "    df_existing = pd.read_csv('ablation_summary.csv', index_col=0)\n",
    "    df_existing = df_existing[~df_existing.index.str.contains('EWKM')]\n",
    "    df_combined = pd.concat([df_existing, df_ewkm])\n",
    "    df_combined.to_csv('ablation_summary.csv')\n",
    "    print(\"Updated ablation_summary.csv\")\n",
    "else:\n",
    "    df_ewkm.to_csv('ewkm_ablation_summary.csv')\n",
    "    print(\"Saved to ewkm_ablation_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Standard Baselines\n",
    "\n",
    "Runs classical ML baselines:\n",
    "1. XGBoost\n",
    "2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    run_baseline_experiment()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Consolidated Results\n",
    "\n",
    "Load and display the summary CSVs generated by the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "if os.path.exists('ablation_summary.csv'):\n",
    "    df_ablation = pd.read_csv('ablation_summary.csv', index_col=0)\n",
    "    print(\"Ablation Results Loaded\")\n",
    "    results.append(df_ablation)\n",
    "\n",
    "if os.path.exists('baseline_summary.csv'):\n",
    "    df_baseline = pd.read_csv('baseline_summary.csv', index_col=0)\n",
    "    print(\"Baseline Results Loaded\")\n",
    "    results.append(df_baseline)\n",
    "    \n",
    "if results:\n",
    "    final_df = pd.concat(results)\n",
    "    display(final_df)\n",
    "else:\n",
    "    print(\"No results files found. Ensure experiments ran successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
