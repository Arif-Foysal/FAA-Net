{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb5f17c",
   "metadata": {},
   "source": [
    "# DyGAT-FR: Dynamic Graph Attention Network with Feedback Refinement\n",
    "## Comprehensive Experiments for Journal Submission\n",
    "\n",
    "This notebook contains all experiments, evaluations, and artifact generation for the DyGAT-FR paper.\n",
    "\n",
    "**Paper Title**: DyGAT-FR: Dynamic Graph Attention Network with Feedback Refinement for Incremental Imbalanced Learning\n",
    "\n",
    "---\n",
    "\n",
    "### Contents\n",
    "1. Environment Setup & Installation\n",
    "2. Dataset Loading & Graph Construction\n",
    "3. Model Training (Static & Incremental)\n",
    "4. Ablation Studies\n",
    "5. Baseline Comparisons\n",
    "6. Evaluation & Metrics\n",
    "7. Figure Generation (Publication Quality)\n",
    "8. Table Generation (LaTeX)\n",
    "9. Statistical Analysis\n",
    "10. Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86dea7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23482c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Mount Google Drive for persistent storage\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!rm -rf FAA-NET\n",
    "!git clone https://github.com/arif-foysal/FAA-NET.git\n",
    "%cd FAA-NET\n",
    "!git checkout followup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (optimized for Colab - uses pre-installed PyTorch)\n",
    "# Skip PyTorch install since Colab already has it\n",
    "\n",
    "# Install PyG - will use pre-built wheels matching Colab's PyTorch\n",
    "!pip install -q torch-geometric\n",
    "\n",
    "# Install other packages in a single command (faster)\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn xgboost lightgbm tabulate\n",
    "\n",
    "# NOTE: torch-scatter and torch-sparse are OPTIONAL\n",
    "# Basic PyG operations (GATConv, etc.) work without them\n",
    "# Only uncomment if you get import errors requiring them:\n",
    "# !pip install -q torch-scatter torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42064cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576db077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DyGAT-FR modules\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from core.dygat_fr import DyGATFR, DyGATFRLoss, DyGATFRTrainer\n",
    "from core.dygat_fr.modules import (\n",
    "    GraphFocalModulation, DyGATConv, \n",
    "    GraphPrototypeAttention, FeedbackRefinementModule,\n",
    "    MinorityReplayBuffer\n",
    ")\n",
    "from core.dygat_fr.data_loader import (\n",
    "    TabularToGraphConverter, TemporalGraphSplitter,\n",
    "    NIDSDataLoader, create_synthetic_graph, compute_graph_statistics\n",
    ")\n",
    "from core.dygat_fr.trainer import IncrementalTrainer\n",
    "from core.dygat_fr.utils import set_seed, compute_metrics\n",
    "\n",
    "# Also import original FAA-Net for comparison\n",
    "from core.model import EDANv3\n",
    "from core.loss import ImbalanceAwareFocalLoss  # Fixed: was ImbalancedFocalLoss\n",
    "\n",
    "print(\"DyGAT-FR modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and output directories\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR = 'results/journal_experiments'\n",
    "FIGURES_DIR = f'{OUTPUT_DIR}/figures'\n",
    "TABLES_DIR = f'{OUTPUT_DIR}/tables'\n",
    "CHECKPOINTS_DIR = f'{OUTPUT_DIR}/checkpoints'\n",
    "\n",
    "for d in [OUTPUT_DIR, FIGURES_DIR, TABLES_DIR, CHECKPOINTS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2e45f",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading & Preprocessing\n",
    "\n",
    "Using identical preprocessing pipeline as FAA-Net for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPROCESSING PIPELINE (Identical to FAA-Net)\n",
    "# ============================================================\n",
    "# Features dropped due to high correlation (> 0.95) - same as FAA-Net\n",
    "DROPPED_FEATURES = [\n",
    "    'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_src_dport_ltm', 'ct_srv_dst',\n",
    "    'dbytes', 'dloss', 'dwin', 'sbytes', 'sloss'\n",
    "]\n",
    "\n",
    "def preprocess_unsw_faanet(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Preprocess UNSW-NB15 using identical pipeline as FAA-Net.\n",
    "    \n",
    "    Steps:\n",
    "    1. Drop ID column\n",
    "    2. Handle '-' in service column\n",
    "    3. Replace inf with NaN\n",
    "    4. Median imputation (fit on train only)\n",
    "    5. Safe label encoding for categoricals\n",
    "    6. Drop high-correlation features\n",
    "    7. StandardScaler (fit on train only)\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test (numpy arrays)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    \n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # 1. Drop ID column\n",
    "    for df in [train_df, test_df]:\n",
    "        if 'id' in df.columns:\n",
    "            df.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    # 2. Handle '-' in service column\n",
    "    for df in [train_df, test_df]:\n",
    "        if 'service' in df.columns:\n",
    "            df['service'] = df['service'].replace('-', 'none')\n",
    "    \n",
    "    # 3. Replace infinite values with NaN\n",
    "    for df in [train_df, test_df]:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 4. Median imputation - fit on train only\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    train_medians = train_df[numeric_cols].median()\n",
    "    train_df[numeric_cols] = train_df[numeric_cols].fillna(train_medians)\n",
    "    test_df[numeric_cols] = test_df[numeric_cols].fillna(train_medians)  # Use train medians!\n",
    "    \n",
    "    # 5. Label encoding for categorical features (safe for unseen labels)\n",
    "    categorical_features = ['proto', 'service', 'state']\n",
    "    \n",
    "    def safe_transform(encoder, series):\n",
    "        \"\"\"Handle unseen labels by mapping to 0.\"\"\"\n",
    "        mapping = {label: idx for idx, label in enumerate(encoder.classes_)}\n",
    "        return series.astype(str).map(mapping).fillna(0).astype(int)\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        if col in train_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(train_df[col].astype(str))\n",
    "            train_df[col] = le.transform(train_df[col].astype(str))\n",
    "            test_df[col] = safe_transform(le, test_df[col])\n",
    "    \n",
    "    # 6. Drop high-correlation features (same as FAA-Net)\n",
    "    train_df.drop(columns=DROPPED_FEATURES, errors='ignore', inplace=True)\n",
    "    test_df.drop(columns=DROPPED_FEATURES, errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. Separate features and labels\n",
    "    drop_cols = ['label', 'attack_cat']\n",
    "    \n",
    "    X_train = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "    y_train = train_df['label'].values if 'label' in train_df.columns else (train_df['attack_cat'] != 'Normal').astype(int).values\n",
    "    \n",
    "    X_test = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "    y_test = test_df['label'].values if 'label' in test_df.columns else (test_df['attack_cat'] != 'Normal').astype(int).values\n",
    "    \n",
    "    # Store feature names\n",
    "    feature_names = list(X_train.columns)\n",
    "    \n",
    "    # 8. StandardScaler - fit on train only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "    X_test_scaled = scaler.transform(X_test.values)\n",
    "    \n",
    "    print(f\"Preprocessing complete (FAA-Net pipeline):\")\n",
    "    print(f\"  Train shape: {X_train_scaled.shape}\")\n",
    "    print(f\"  Test shape: {X_test_scaled.shape}\")\n",
    "    print(f\"  Features: {len(feature_names)}\")\n",
    "    print(f\"  Dropped features: {DROPPED_FEATURES}\")\n",
    "    print(f\"  Train class distribution: Normal={np.sum(y_train==0):,}, Attack={np.sum(y_train==1):,}\")\n",
    "    print(f\"  Test class distribution: Normal={np.sum(y_test==0):,}, Attack={np.sum(y_test==1):,}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_names, scaler\n",
    "\n",
    "print(\"Preprocessing functions defined (identical to FAA-Net).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba81218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download UNSW-NB15 dataset if not present\n",
    "# You can also upload manually to Colab\n",
    "\n",
    "TRAIN_FILE = 'UNSW_NB15_training-set.csv'\n",
    "TEST_FILE = 'UNSW_NB15_testing-set.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(TRAIN_FILE):\n",
    "    print(\"Dataset not found. Please upload UNSW-NB15 dataset files.\")\n",
    "    print(\"Expected files: UNSW_NB15_training-set.csv, UNSW_NB15_testing-set.csv\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        print(\"\\nUpload the dataset files:\")\n",
    "        uploaded = files.upload()\n",
    "else:\n",
    "    print(f\"Dataset found: {TRAIN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Use synthetic data for testing (comment out if using real data)\n",
    "USE_SYNTHETIC = True  # Set to False to use UNSW-NB15\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    print(\"Using synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Create larger synthetic dataset\n",
    "    train_data = create_synthetic_graph(\n",
    "        n_nodes=10000,\n",
    "        n_features=33,\n",
    "        minority_ratio=0.15,\n",
    "        k_neighbors=10,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    test_data = create_synthetic_graph(\n",
    "        n_nodes=3000,\n",
    "        n_features=33,\n",
    "        minority_ratio=0.20,  # Different ratio to simulate drift\n",
    "        k_neighbors=10,\n",
    "        random_state=SEED + 1\n",
    "    )\n",
    "    \n",
    "    # Create train/val masks\n",
    "    n_train = train_data.num_nodes\n",
    "    perm = torch.randperm(n_train)\n",
    "    train_data.train_mask = torch.zeros(n_train, dtype=torch.bool)\n",
    "    train_data.val_mask = torch.zeros(n_train, dtype=torch.bool)\n",
    "    train_data.train_mask[perm[:int(0.8*n_train)]] = True\n",
    "    train_data.val_mask[perm[int(0.8*n_train):]] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Loading UNSW-NB15 dataset with FAA-Net preprocessing...\")\n",
    "    \n",
    "    # Load raw CSVs\n",
    "    train_df = pd.read_csv(TRAIN_FILE)\n",
    "    test_df = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    print(f\"Raw train shape: {train_df.shape}\")\n",
    "    print(f\"Raw test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Apply FAA-Net preprocessing pipeline\n",
    "    X_train, X_test, y_train, y_test, feature_names, scaler = preprocess_unsw_faanet(train_df, test_df)\n",
    "    \n",
    "    # Convert to graph format using TabularToGraphConverter\n",
    "    print(\"\\nConverting to graph format...\")\n",
    "    graph_converter = TabularToGraphConverter(k_neighbors=10, metric='euclidean')\n",
    "    \n",
    "    # Fit on training data, transform both\n",
    "    graph_converter.fit(X_train)\n",
    "    train_data = graph_converter.transform(X_train, y_train)\n",
    "    test_data = graph_converter.transform(X_test, y_test)\n",
    "    \n",
    "    # Create train/val split\n",
    "    n_train = train_data.num_nodes\n",
    "    perm = torch.randperm(n_train)\n",
    "    train_data.train_mask = torch.zeros(n_train, dtype=torch.bool)\n",
    "    train_data.val_mask = torch.zeros(n_train, dtype=torch.bool)\n",
    "    train_data.train_mask[perm[:int(0.8*n_train)]] = True\n",
    "    train_data.val_mask[perm[int(0.8*n_train):]] = True\n",
    "    \n",
    "    print(f\"Graph conversion complete.\")\n",
    "    print(f\"  Train nodes: {train_data.num_nodes:,}, edges: {train_data.edge_index.size(1):,}\")\n",
    "    print(f\"  Test nodes: {test_data.num_nodes:,}, edges: {test_data.edge_index.size(1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, data in [('Training', train_data), ('Test', test_data)]:\n",
    "    stats = compute_graph_statistics(data)\n",
    "    print(f\"\\n{name} Set:\")\n",
    "    print(f\"  Nodes: {stats['num_nodes']:,}\")\n",
    "    print(f\"  Edges: {stats['num_edges']:,}\")\n",
    "    print(f\"  Features: {stats['num_features']}\")\n",
    "    print(f\"  Avg Degree: {stats['avg_degree']:.2f}\")\n",
    "    print(f\"  Class Distribution: {stats['class_distribution']}\")\n",
    "    if 'imbalance_ratio' in stats:\n",
    "        print(f\"  Imbalance Ratio: {stats['imbalance_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal increments for incremental learning experiments\n",
    "N_INCREMENTS = 5\n",
    "\n",
    "splitter = TemporalGraphSplitter(\n",
    "    n_increments=N_INCREMENTS,\n",
    "    minority_drift=True,\n",
    "    drift_intensity=0.3\n",
    ")\n",
    "\n",
    "increments = splitter.split(train_data, random_state=SEED)\n",
    "\n",
    "print(f\"\\nCreated {len(increments)} temporal increments:\")\n",
    "for i, inc in enumerate(increments):\n",
    "    minority_count = (inc.y == 1).sum().item()\n",
    "    majority_count = (inc.y == 0).sum().item()\n",
    "    print(f\"  Increment {i+1}: {inc.num_nodes} nodes \"\n",
    "          f\"(Minority: {minority_count}, Majority: {majority_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76f91b",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "MODEL_CONFIG = {\n",
    "    'in_channels': train_data.x.size(1),\n",
    "    'hidden_channels': 128,\n",
    "    'out_channels': 64,\n",
    "    'num_classes': 2,\n",
    "    'num_layers': 3,\n",
    "    'heads': 4,\n",
    "    'n_prototypes': 8,\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 2.0,\n",
    "    'dropout': 0.3,\n",
    "    'prototype_momentum': 0.9,\n",
    "    'use_feedback': True\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 100,\n",
    "    'curriculum_epochs': 10,\n",
    "    'early_stopping_patience': 15\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DyGAT-FR model\n",
    "model = DyGATFR(**MODEL_CONFIG).to(DEVICE)\n",
    "\n",
    "print(f\"\\nModel Parameters: {model.count_parameters():,}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and trainer\n",
    "loss_fn = DyGATFRLoss(\n",
    "    focal_alpha=MODEL_CONFIG['focal_alpha'],\n",
    "    focal_gamma=MODEL_CONFIG['focal_gamma'],\n",
    "    contrastive_weight=0.1,\n",
    "    replay_weight=0.1,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "trainer = DyGATFRTrainer(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    "    lr=TRAINING_CONFIG['lr'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "    curriculum_epochs=TRAINING_CONFIG['curriculum_epochs']\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DyGAT-FR on full training data (static setting)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING DyGAT-FR (Static Setting)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.train_increment(\n",
    "    train_data,\n",
    "    epochs=TRAINING_CONFIG['epochs'],\n",
    "    train_mask=train_data.train_mask,\n",
    "    val_mask=train_data.val_mask,\n",
    "    verbose=True,\n",
    "    early_stopping_patience=TRAINING_CONFIG['early_stopping_patience']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef153d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save static model checkpoint\n",
    "static_checkpoint_path = f\"{CHECKPOINTS_DIR}/dygat_fr_static.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "    'history': history\n",
    "}, static_checkpoint_path)\n",
    "print(f\"Static model saved to {static_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d31d6",
   "metadata": {},
   "source": [
    "### 3.1 Incremental Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DyGAT-FR with incremental learning\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING DyGAT-FR (Incremental Setting)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create fresh model for incremental training\n",
    "model_incremental = DyGATFR(**MODEL_CONFIG).to(DEVICE)\n",
    "\n",
    "incremental_trainer = IncrementalTrainer(\n",
    "    model=model_incremental,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    "    lr=TRAINING_CONFIG['lr'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "    curriculum_epochs=TRAINING_CONFIG['curriculum_epochs'] // 2\n",
    ")\n",
    "\n",
    "# Train on increments\n",
    "incremental_results = incremental_trainer.train_increments(\n",
    "    increments,\n",
    "    epochs_per_increment=TRAINING_CONFIG['epochs'] // N_INCREMENTS,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nIncremental training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590003e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save incremental model checkpoint\n",
    "incremental_checkpoint_path = f\"{CHECKPOINTS_DIR}/dygat_fr_incremental.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model_incremental.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "    'incremental_results': incremental_results\n",
    "}, incremental_checkpoint_path)\n",
    "print(f\"Incremental model saved to {incremental_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fcf25",
   "metadata": {},
   "source": [
    "## 4. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ablation configurations\n",
    "ABLATION_CONFIGS = {\n",
    "    'DyGAT-FR (Full)': MODEL_CONFIG.copy(),\n",
    "    \n",
    "    'w/o Feedback': {**MODEL_CONFIG, 'use_feedback': False},\n",
    "    \n",
    "    'w/o Prototypes': {**MODEL_CONFIG, 'n_prototypes': 0},\n",
    "    \n",
    "    'w/o Focal Mod': {**MODEL_CONFIG, 'focal_alpha': 0.0},\n",
    "    \n",
    "    'Reduced Heads (2)': {**MODEL_CONFIG, 'heads': 2},\n",
    "    \n",
    "    'Shallow (2 layers)': {**MODEL_CONFIG, 'num_layers': 2},\n",
    "}\n",
    "\n",
    "print(f\"Running {len(ABLATION_CONFIGS)} ablation configurations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ablation experiments\n",
    "ablation_results = {}\n",
    "\n",
    "for name, config in ABLATION_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ablation: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Handle special case for no prototypes\n",
    "    if config.get('n_prototypes', 8) == 0:\n",
    "        config['n_prototypes'] = 1  # Minimum required\n",
    "    \n",
    "    # Create model\n",
    "    abl_model = DyGATFR(**config).to(DEVICE)\n",
    "    \n",
    "    # Create trainer\n",
    "    abl_trainer = DyGATFRTrainer(\n",
    "        model=abl_model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=DEVICE,\n",
    "        lr=TRAINING_CONFIG['lr'],\n",
    "        weight_decay=TRAINING_CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Train (reduced epochs for ablation)\n",
    "    abl_history = abl_trainer.train_increment(\n",
    "        train_data,\n",
    "        epochs=50,  # Reduced for ablation\n",
    "        train_mask=train_data.train_mask,\n",
    "        val_mask=train_data.val_mask,\n",
    "        verbose=False,\n",
    "        early_stopping_patience=10\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = abl_trainer.evaluate(test_data)\n",
    "    ablation_results[name] = {\n",
    "        'config': config,\n",
    "        'history': abl_history,\n",
    "        'metrics': metrics,\n",
    "        'params': abl_model.count_parameters()\n",
    "    }\n",
    "    \n",
    "    print(f\"  F1: {metrics['f1']:.4f} | Recall: {metrics['recall']:.4f} | \"\n",
    "          f\"Precision: {metrics['precision']:.4f} | AUC: {metrics.get('auc', 0):.4f}\")\n",
    "\n",
    "print(\"\\nAblation studies completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647ded7",
   "metadata": {},
   "source": [
    "## 5. Baseline Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7aa4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1: Standard GAT (without focal modulation)\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class BaselineGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels // heads, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels // heads, heads=heads, dropout=dropout)\n",
    "        self.conv3 = GATConv(hidden_channels, out_channels // heads, heads=heads, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(out_channels, 1)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = torch.nn.functional.gelu(self.conv1(x, edge_index))\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = torch.nn.functional.gelu(self.conv2(x, edge_index))\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Baseline GAT defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee432269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2: XGBoost and LightGBM (Tree-based)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def train_tree_baseline(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train and evaluate tree-based baseline.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_prob),\n",
    "        'avg_precision': average_precision_score(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_prob\n",
    "\n",
    "print(\"Tree baseline functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all baselines\n",
    "baseline_results = {}\n",
    "\n",
    "# Prepare data for tree baselines\n",
    "X_train_np = train_data.x.numpy()\n",
    "y_train_np = train_data.y.numpy()\n",
    "X_test_np = test_data.x.numpy()\n",
    "y_test_np = test_data.y.numpy()\n",
    "\n",
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train_np[y_train_np==0]) / len(y_train_np[y_train_np==1]),\n",
    "    random_state=SEED, verbosity=0\n",
    ")\n",
    "xgb_metrics, xgb_pred, xgb_prob = train_tree_baseline(\n",
    "    xgb, X_train_np, y_train_np, X_test_np, y_test_np\n",
    ")\n",
    "baseline_results['XGBoost'] = {'metrics': xgb_metrics, 'pred': xgb_pred, 'prob': xgb_prob}\n",
    "print(f\"  XGBoost - F1: {xgb_metrics['f1']:.4f}, Recall: {xgb_metrics['recall']:.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "    is_unbalance=True, random_state=SEED, verbosity=-1\n",
    ")\n",
    "lgbm_metrics, lgbm_pred, lgbm_prob = train_tree_baseline(\n",
    "    lgbm, X_train_np, y_train_np, X_test_np, y_test_np\n",
    ")\n",
    "baseline_results['LightGBM'] = {'metrics': lgbm_metrics, 'pred': lgbm_pred, 'prob': lgbm_prob}\n",
    "print(f\"  LightGBM - F1: {lgbm_metrics['f1']:.4f}, Recall: {lgbm_metrics['recall']:.4f}\")\n",
    "\n",
    "# Standard GAT\n",
    "print(\"Training Standard GAT...\")\n",
    "gat_model = BaselineGAT(\n",
    "    in_channels=train_data.x.size(1),\n",
    "    hidden_channels=128,\n",
    "    out_channels=64,\n",
    "    heads=4,\n",
    "    dropout=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "gat_optimizer = torch.optim.AdamW(gat_model.parameters(), lr=1e-3)\n",
    "gat_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop for GAT\n",
    "x_gpu = train_data.x.to(DEVICE)\n",
    "edge_gpu = train_data.edge_index.to(DEVICE)\n",
    "y_gpu = train_data.y.float().to(DEVICE)\n",
    "\n",
    "gat_model.train()\n",
    "for epoch in range(50):\n",
    "    gat_optimizer.zero_grad()\n",
    "    out = gat_model(x_gpu, edge_gpu).squeeze()\n",
    "    loss = gat_criterion(out[train_data.train_mask], y_gpu[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    gat_optimizer.step()\n",
    "\n",
    "# Evaluate GAT\n",
    "gat_model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test_gpu = test_data.x.to(DEVICE)\n",
    "    edge_test_gpu = test_data.edge_index.to(DEVICE)\n",
    "    gat_logits = gat_model(x_test_gpu, edge_test_gpu).squeeze()\n",
    "    gat_prob = torch.sigmoid(gat_logits).cpu().numpy()\n",
    "    gat_pred = (gat_prob > 0.5).astype(int)\n",
    "\n",
    "gat_metrics = {\n",
    "    'accuracy': (gat_pred == y_test_np).mean(),\n",
    "    'f1': f1_score(y_test_np, gat_pred),\n",
    "    'recall': recall_score(y_test_np, gat_pred),\n",
    "    'precision': precision_score(y_test_np, gat_pred),\n",
    "    'auc': roc_auc_score(y_test_np, gat_prob),\n",
    "    'avg_precision': average_precision_score(y_test_np, gat_prob)\n",
    "}\n",
    "baseline_results['Standard GAT'] = {'metrics': gat_metrics, 'pred': gat_pred, 'prob': gat_prob}\n",
    "print(f\"  Standard GAT - F1: {gat_metrics['f1']:.4f}, Recall: {gat_metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\nAll baselines completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70a345",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DyGAT-FR on test set\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Static DyGAT-FR\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = test_data.x.to(DEVICE)\n",
    "    edge_test = test_data.edge_index.to(DEVICE)\n",
    "    logits = model(x_test, edge_test)\n",
    "    dygat_prob = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    dygat_pred = (dygat_prob > 0.5).astype(int)\n",
    "\n",
    "dygat_metrics = {\n",
    "    'accuracy': (dygat_pred == y_test_np).mean(),\n",
    "    'f1': f1_score(y_test_np, dygat_pred),\n",
    "    'recall': recall_score(y_test_np, dygat_pred),\n",
    "    'precision': precision_score(y_test_np, dygat_pred),\n",
    "    'auc': roc_auc_score(y_test_np, dygat_prob),\n",
    "    'avg_precision': average_precision_score(y_test_np, dygat_prob)\n",
    "}\n",
    "\n",
    "# Store for comparison\n",
    "all_results = {\n",
    "    'DyGAT-FR (Static)': {'metrics': dygat_metrics, 'pred': dygat_pred, 'prob': dygat_prob},\n",
    "    **baseline_results\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Model':<20} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'AUC':>10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    m = result['metrics']\n",
    "    print(f\"{name:<20} {m['accuracy']:>10.4f} {m['precision']:>10.4f} \"\n",
    "          f\"{m['recall']:>10.4f} {m['f1']:>10.4f} {m['auc']:>10.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    cm = confusion_matrix(y_test_np, result['pred'])\n",
    "    confusion_matrices[name] = cm\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  TP: {tp:,} | FP: {fp:,}\")\n",
    "    print(f\"  FN: {fn:,} | TN: {tn:,}\")\n",
    "    print(f\"  FPR: {fp/(fp+tn):.4f} | FNR: {fn/(fn+tp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec09c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental learning evaluation - forgetting analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INCREMENTAL LEARNING - FORGETTING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate incremental model on each increment\n",
    "model_incremental.eval()\n",
    "forgetting_metrics = []\n",
    "\n",
    "for i, inc in enumerate(increments):\n",
    "    with torch.no_grad():\n",
    "        x_inc = inc.x.to(DEVICE)\n",
    "        edge_inc = inc.edge_index.to(DEVICE)\n",
    "        logits = model_incremental(x_inc, edge_inc)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "    \n",
    "    y_inc = inc.y.numpy()\n",
    "    f1 = f1_score(y_inc, pred)\n",
    "    recall = recall_score(y_inc, pred)\n",
    "    \n",
    "    forgetting_metrics.append({\n",
    "        'increment': i + 1,\n",
    "        'f1': f1,\n",
    "        'recall': recall\n",
    "    })\n",
    "    \n",
    "    print(f\"Increment {i+1}: F1={f1:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "# Calculate forgetting\n",
    "f1_values = [m['f1'] for m in forgetting_metrics]\n",
    "forgetting = max(f1_values) - f1_values[-1]\n",
    "print(f\"\\nForgetting (max F1 - final F1): {forgetting:.4f} ({forgetting*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bff263",
   "metadata": {},
   "source": [
    "## 7. Figure Generation (Publication Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Training Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train', color='#2196F3', linewidth=2)\n",
    "if 'val_loss' in history:\n",
    "    ax.plot(history['val_loss'], label='Validation', color='#FF5722', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('(a) Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_f1'], label='Train', color='#2196F3', linewidth=2)\n",
    "if 'val_f1' in history:\n",
    "    ax.plot(history['val_f1'], label='Validation', color='#FF5722', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('(b) F1 Score')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "ax = axes[2]\n",
    "ax.plot(history['train_recall'], label='Train', color='#2196F3', linewidth=2)\n",
    "if 'val_recall' in history:\n",
    "    ax.plot(history['val_recall'], label='Validation', color='#FF5722', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('(c) Recall')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig1_training_curves.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig1_training_curves.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig1_training_curves.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: ROC Curves Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = ['#2196F3', '#4CAF50', '#FF5722', '#9C27B0', '#795548']\n",
    "\n",
    "# ROC Curves\n",
    "ax = axes[0]\n",
    "for i, (name, result) in enumerate(all_results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test_np, result['prob'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\", \n",
    "            color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('(a) ROC Curves')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curves\n",
    "ax = axes[1]\n",
    "for i, (name, result) in enumerate(all_results.items()):\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test_np, result['prob'])\n",
    "    ap = average_precision_score(y_test_np, result['prob'])\n",
    "    ax.plot(recall_vals, precision_vals, label=f\"{name} (AP={ap:.3f})\",\n",
    "            color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('(b) Precision-Recall Curves')\n",
    "ax.legend(loc='lower left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig2_roc_pr_curves.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig2_roc_pr_curves.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig2_roc_pr_curves.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "class_names = ['Normal', 'Attack']\n",
    "selected_models = ['DyGAT-FR (Static)', 'Standard GAT', 'XGBoost', 'LightGBM']\n",
    "\n",
    "for ax, name in zip(axes, selected_models):\n",
    "    cm = confusion_matrices[name]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 12})\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig3_confusion_matrices.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig3_confusion_matrices.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig3_confusion_matrices.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Ablation Study Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ablation_names = list(ablation_results.keys())\n",
    "f1_scores = [ablation_results[name]['metrics']['f1'] for name in ablation_names]\n",
    "recall_scores = [ablation_results[name]['metrics']['recall'] for name in ablation_names]\n",
    "\n",
    "x = np.arange(len(ablation_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, f1_scores, width, label='F1 Score', color='#2196F3')\n",
    "bars2 = ax.bar(x + width/2, recall_scores, width, label='Recall', color='#4CAF50')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Ablation Study Results')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ablation_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig4_ablation_study.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig4_ablation_study.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig4_ablation_study.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Incremental Learning Performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "increments_x = [m['increment'] for m in forgetting_metrics]\n",
    "f1_vals = [m['f1'] for m in forgetting_metrics]\n",
    "recall_vals = [m['recall'] for m in forgetting_metrics]\n",
    "\n",
    "# F1 across increments\n",
    "ax = axes[0]\n",
    "ax.plot(increments_x, f1_vals, 'o-', color='#2196F3', linewidth=2, markersize=10)\n",
    "ax.fill_between(increments_x, f1_vals, alpha=0.2, color='#2196F3')\n",
    "ax.set_xlabel('Increment')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('(a) F1 Score Across Increments')\n",
    "ax.set_xticks(increments_x)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (inc, f1) in enumerate(zip(increments_x, f1_vals)):\n",
    "    ax.annotate(f'{f1:.3f}', (inc, f1), textcoords=\"offset points\",\n",
    "                xytext=(0, 10), ha='center', fontsize=10)\n",
    "\n",
    "# Recall across increments\n",
    "ax = axes[1]\n",
    "ax.plot(increments_x, recall_vals, 'o-', color='#4CAF50', linewidth=2, markersize=10)\n",
    "ax.fill_between(increments_x, recall_vals, alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Increment')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('(b) Recall Across Increments')\n",
    "ax.set_xticks(increments_x)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (inc, rec) in enumerate(zip(increments_x, recall_vals)):\n",
    "    ax.annotate(f'{rec:.3f}', (inc, rec), textcoords=\"offset points\",\n",
    "                xytext=(0, 10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig5_incremental_performance.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig5_incremental_performance.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig5_incremental_performance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: Model Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models = list(all_results.keys())\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "x = np.arange(len(models))\n",
    "width = 0.15\n",
    "\n",
    "colors = ['#1976D2', '#388E3C', '#F57C00', '#7B1FA2', '#C62828']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = [all_results[m]['metrics'][metric] for m in models]\n",
    "    bars = ax.bar(x + i*width, values, width, label=metric.upper(), color=colors[i])\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax.legend(loc='upper right', ncol=5)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}/fig6_model_comparison.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{FIGURES_DIR}/fig6_model_comparison.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR}/fig6_model_comparison.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6265bee",
   "metadata": {},
   "source": [
    "## 8. Table Generation (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Main Results Comparison\n",
    "def generate_latex_table(results_dict, caption, label):\n",
    "    \"\"\"Generate LaTeX table from results.\"\"\"\n",
    "    latex = []\n",
    "    latex.append(r\"\\begin{table}[htbp]\")\n",
    "    latex.append(r\"\\centering\")\n",
    "    latex.append(r\"\\caption{\" + caption + r\"}\")\n",
    "    latex.append(r\"\\label{\" + label + r\"}\")\n",
    "    latex.append(r\"\\begin{tabular}{lccccc}\")\n",
    "    latex.append(r\"\\toprule\")\n",
    "    latex.append(r\"Model & Accuracy & Precision & Recall & F1 & AUC \\\\\")\n",
    "    latex.append(r\"\\midrule\")\n",
    "    \n",
    "    # Find best values for highlighting\n",
    "    best_f1 = max(r['metrics']['f1'] for r in results_dict.values())\n",
    "    best_recall = max(r['metrics']['recall'] for r in results_dict.values())\n",
    "    \n",
    "    for name, result in results_dict.items():\n",
    "        m = result['metrics']\n",
    "        # Bold best F1 and Recall\n",
    "        f1_str = f\"\\\\textbf{{{m['f1']:.4f}}}\" if m['f1'] == best_f1 else f\"{m['f1']:.4f}\"\n",
    "        recall_str = f\"\\\\textbf{{{m['recall']:.4f}}}\" if m['recall'] == best_recall else f\"{m['recall']:.4f}\"\n",
    "        \n",
    "        latex.append(f\"{name} & {m['accuracy']:.4f} & {m['precision']:.4f} & \"\n",
    "                    f\"{recall_str} & {f1_str} & {m['auc']:.4f} \\\\\\\\\")\n",
    "    \n",
    "    latex.append(r\"\\bottomrule\")\n",
    "    latex.append(r\"\\end{tabular}\")\n",
    "    latex.append(r\"\\end{table}\")\n",
    "    \n",
    "    return \"\\n\".join(latex)\n",
    "\n",
    "# Generate main results table\n",
    "table1_latex = generate_latex_table(\n",
    "    all_results,\n",
    "    \"Comparison of DyGAT-FR with baseline methods on test set.\",\n",
    "    \"tab:main_results\"\n",
    ")\n",
    "\n",
    "print(\"Table 1: Main Results\")\n",
    "print(\"=\" * 60)\n",
    "print(table1_latex)\n",
    "\n",
    "# Save to file\n",
    "with open(f'{TABLES_DIR}/table1_main_results.tex', 'w') as f:\n",
    "    f.write(table1_latex)\n",
    "print(f\"\\nSaved: {TABLES_DIR}/table1_main_results.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd602dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Ablation Study Results\n",
    "ablation_for_table = {name: {'metrics': result['metrics']} \n",
    "                      for name, result in ablation_results.items()}\n",
    "\n",
    "table2_latex = generate_latex_table(\n",
    "    ablation_for_table,\n",
    "    \"Ablation study results showing contribution of each component.\",\n",
    "    \"tab:ablation\"\n",
    ")\n",
    "\n",
    "print(\"\\nTable 2: Ablation Study\")\n",
    "print(\"=\" * 60)\n",
    "print(table2_latex)\n",
    "\n",
    "with open(f'{TABLES_DIR}/table2_ablation.tex', 'w') as f:\n",
    "    f.write(table2_latex)\n",
    "print(f\"\\nSaved: {TABLES_DIR}/table2_ablation.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Incremental Learning Results\n",
    "latex = []\n",
    "latex.append(r\"\\begin{table}[htbp]\")\n",
    "latex.append(r\"\\centering\")\n",
    "latex.append(r\"\\caption{Performance across incremental updates.}\")\n",
    "latex.append(r\"\\label{tab:incremental}\")\n",
    "latex.append(r\"\\begin{tabular}{ccc}\")\n",
    "latex.append(r\"\\toprule\")\n",
    "latex.append(r\"Increment & F1 Score & Recall \\\\\")\n",
    "latex.append(r\"\\midrule\")\n",
    "\n",
    "for m in forgetting_metrics:\n",
    "    latex.append(f\"{m['increment']} & {m['f1']:.4f} & {m['recall']:.4f} \\\\\\\\\")\n",
    "\n",
    "latex.append(r\"\\midrule\")\n",
    "latex.append(f\"Forgetting & \\\\multicolumn{{2}}{{c}}{{{forgetting:.4f} ({forgetting*100:.2f}\\%)}} \\\\\\\\\")\n",
    "latex.append(r\"\\bottomrule\")\n",
    "latex.append(r\"\\end{tabular}\")\n",
    "latex.append(r\"\\end{table}\")\n",
    "\n",
    "table3_latex = \"\\n\".join(latex)\n",
    "\n",
    "print(\"\\nTable 3: Incremental Learning\")\n",
    "print(\"=\" * 60)\n",
    "print(table3_latex)\n",
    "\n",
    "with open(f'{TABLES_DIR}/table3_incremental.tex', 'w') as f:\n",
    "    f.write(table3_latex)\n",
    "print(f\"\\nSaved: {TABLES_DIR}/table3_incremental.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4: Model Complexity Comparison\n",
    "complexity_data = {\n",
    "    'DyGAT-FR': model.count_parameters(),\n",
    "    'Standard GAT': gat_model.count_parameters(),\n",
    "}\n",
    "\n",
    "latex = []\n",
    "latex.append(r\"\\begin{table}[htbp]\")\n",
    "latex.append(r\"\\centering\")\n",
    "latex.append(r\"\\caption{Model complexity comparison.}\")\n",
    "latex.append(r\"\\label{tab:complexity}\")\n",
    "latex.append(r\"\\begin{tabular}{lcc}\")\n",
    "latex.append(r\"\\toprule\")\n",
    "latex.append(r\"Model & Parameters & Type \\\\\")\n",
    "latex.append(r\"\\midrule\")\n",
    "latex.append(f\"DyGAT-FR & {complexity_data['DyGAT-FR']:,} & Graph Neural Network \\\\\\\\\")\n",
    "latex.append(f\"Standard GAT & {complexity_data['Standard GAT']:,} & Graph Neural Network \\\\\\\\\")\n",
    "latex.append(r\"XGBoost & N/A & Gradient Boosting \\\\\")\n",
    "latex.append(r\"LightGBM & N/A & Gradient Boosting \\\\\")\n",
    "latex.append(r\"\\bottomrule\")\n",
    "latex.append(r\"\\end{tabular}\")\n",
    "latex.append(r\"\\end{table}\")\n",
    "\n",
    "table4_latex = \"\\n\".join(latex)\n",
    "print(\"\\nTable 4: Model Complexity\")\n",
    "print(\"=\" * 60)\n",
    "print(table4_latex)\n",
    "\n",
    "with open(f'{TABLES_DIR}/table4_complexity.tex', 'w') as f:\n",
    "    f.write(table4_latex)\n",
    "print(f\"\\nSaved: {TABLES_DIR}/table4_complexity.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50a896",
   "metadata": {},
   "source": [
    "## 9. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar's Test for comparing classifiers\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def mcnemar_test(y_true, pred1, pred2):\n",
    "    \"\"\"Perform McNemar's test to compare two classifiers.\"\"\"\n",
    "    # Build contingency table\n",
    "    correct1 = (pred1 == y_true)\n",
    "    correct2 = (pred2 == y_true)\n",
    "    \n",
    "    # b: pred1 correct, pred2 wrong\n",
    "    # c: pred1 wrong, pred2 correct\n",
    "    b = np.sum(correct1 & ~correct2)\n",
    "    c = np.sum(~correct1 & correct2)\n",
    "    \n",
    "    # McNemar's statistic with continuity correction\n",
    "    if b + c > 0:\n",
    "        chi2_stat = (abs(b - c) - 1) ** 2 / (b + c)\n",
    "        p_value = 1 - chi2.cdf(chi2_stat, df=1)\n",
    "    else:\n",
    "        chi2_stat = 0\n",
    "        p_value = 1.0\n",
    "    \n",
    "    return chi2_stat, p_value, b, c\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dygat_pred = all_results['DyGAT-FR (Static)']['pred']\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if name != 'DyGAT-FR (Static)':\n",
    "        chi2_stat, p_value, b, c = mcnemar_test(y_test_np, dygat_pred, result['pred'])\n",
    "        sig = \"*\" if p_value < 0.05 else \"\"\n",
    "        sig = \"**\" if p_value < 0.01 else sig\n",
    "        sig = \"***\" if p_value < 0.001 else sig\n",
    "        print(f\"DyGAT-FR vs {name}: ={chi2_stat:.4f}, p={p_value:.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size calculation (Cohen's h for proportions)\n",
    "def cohens_h(p1, p2):\n",
    "    \"\"\"Calculate Cohen's h effect size for two proportions.\"\"\"\n",
    "    import math\n",
    "    phi1 = 2 * math.asin(math.sqrt(p1))\n",
    "    phi2 = 2 * math.asin(math.sqrt(p2))\n",
    "    return phi1 - phi2\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EFFECT SIZE (Cohen's h) for Recall Improvement\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dygat_recall = all_results['DyGAT-FR (Static)']['metrics']['recall']\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if name != 'DyGAT-FR (Static)':\n",
    "        baseline_recall = result['metrics']['recall']\n",
    "        h = cohens_h(dygat_recall, baseline_recall)\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(h) < 0.2:\n",
    "            interpretation = \"small\"\n",
    "        elif abs(h) < 0.5:\n",
    "            interpretation = \"medium\"\n",
    "        else:\n",
    "            interpretation = \"large\"\n",
    "        \n",
    "        print(f\"vs {name}: h={h:.4f} ({interpretation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab18084",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda24d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results into a single JSON file\n",
    "export_data = {\n",
    "    'experiment_info': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'dataset': 'Synthetic' if USE_SYNTHETIC else 'UNSW-NB15',\n",
    "        'device': str(DEVICE),\n",
    "        'seed': SEED\n",
    "    },\n",
    "    'model_config': MODEL_CONFIG,\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'main_results': {name: result['metrics'] for name, result in all_results.items()},\n",
    "    'ablation_results': {name: result['metrics'] for name, result in ablation_results.items()},\n",
    "    'incremental_results': forgetting_metrics,\n",
    "    'forgetting_metric': forgetting,\n",
    "    'model_parameters': model.count_parameters()\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "results_json_path = f'{OUTPUT_DIR}/all_results.json'\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"All results exported to {results_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d45752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary CSV\n",
    "summary_rows = []\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    row = {'Model': name, **result['metrics']}\n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv_path = f'{OUTPUT_DIR}/results_summary.csv'\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"Results summary saved to {summary_csv_path}\")\n",
    "print(\"\\n\" + summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86290aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated artifacts\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATED ARTIFACTS FOR JOURNAL PAPER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFigures:\")\n",
    "for f in sorted(os.listdir(FIGURES_DIR)):\n",
    "    print(f\"  - {FIGURES_DIR}/{f}\")\n",
    "\n",
    "print(\"\\nTables (LaTeX):\")\n",
    "for f in sorted(os.listdir(TABLES_DIR)):\n",
    "    print(f\"  - {TABLES_DIR}/{f}\")\n",
    "\n",
    "print(\"\\nCheckpoints:\")\n",
    "for f in sorted(os.listdir(CHECKPOINTS_DIR)):\n",
    "    print(f\"  - {CHECKPOINTS_DIR}/{f}\")\n",
    "\n",
    "print(\"\\nData Files:\")\n",
    "print(f\"  - {results_json_path}\")\n",
    "print(f\"  - {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (Colab only)\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    \n",
    "    # Zip results folder\n",
    "    shutil.make_archive('journal_results', 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    # Download\n",
    "    from google.colab import files\n",
    "    files.download('journal_results.zip')\n",
    "    \n",
    "    print(\"\\nResults downloaded as journal_results.zip\")\n",
    "else:\n",
    "    print(f\"\\nResults saved locally in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f07267",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has generated all artifacts required for the DyGAT-FR journal paper:\n",
    "\n",
    "### Figures\n",
    "1. **fig1_training_curves.pdf** - Training loss, F1, and recall curves\n",
    "2. **fig2_roc_pr_curves.pdf** - ROC and Precision-Recall curves comparison\n",
    "3. **fig3_confusion_matrices.pdf** - Confusion matrices for all models\n",
    "4. **fig4_ablation_study.pdf** - Ablation study bar chart\n",
    "5. **fig5_incremental_performance.pdf** - Performance across increments\n",
    "6. **fig6_model_comparison.pdf** - Overall model comparison\n",
    "\n",
    "### Tables (LaTeX)\n",
    "1. **table1_main_results.tex** - Main comparison results\n",
    "2. **table2_ablation.tex** - Ablation study results\n",
    "3. **table3_incremental.tex** - Incremental learning results\n",
    "4. **table4_complexity.tex** - Model complexity comparison\n",
    "\n",
    "### Statistical Analysis\n",
    "- McNemar's test for classifier comparison\n",
    "- Cohen's h effect size for recall improvement\n",
    "\n",
    "### Model Checkpoints\n",
    "- Static trained model\n",
    "- Incrementally trained model\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run on full UNSW-NB15 dataset (set `USE_SYNTHETIC = False`)\n",
    "2. Add additional datasets (CIC-IDS2017, ToN-IoT)\n",
    "3. Run multiple seeds for confidence intervals\n",
    "4. Generate architecture diagram"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
